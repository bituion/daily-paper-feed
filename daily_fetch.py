import arxiv
import json
import os
import random
import datetime
import time
from openai import OpenAI

# 1. é…ç½®
TARGET_CATEGORIES = ["cs.AI", "cs.CL", "cs.LG", "cs.CV"]
JSON_FILE = "papers_data.json"
API_KEY = os.environ.get("DEEPSEEK_API_KEY") # ä»ç¯å¢ƒå˜é‡è·å– Key
BASE_URL = "https://api.deepseek.com"

# é¢œè‰²æ± 
GRADIENTS = [
    "bg-gradient-to-br from-indigo-600 to-blue-600",
    "bg-gradient-to-br from-purple-500 to-pink-600",
    "bg-gradient-to-br from-orange-400 to-red-500",
    "bg-gradient-to-br from-emerald-500 to-teal-600",
    "bg-gradient-to-br from-slate-700 to-slate-900",
    "bg-gradient-to-br from-rose-500 to-red-600",
]

if not API_KEY:
    raise ValueError("æœªæ£€æµ‹åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼")

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

def get_existing_papers():
    """è¯»å–å·²æœ‰çš„ JSON æ•°æ®"""
    if os.path.exists(JSON_FILE):
        try:
            with open(JSON_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

def fetch_arxiv_updates(existing_ids):
    """æŠ“å– ArXiv æ–°è®ºæ–‡"""
    print("ğŸš€ æ­£åœ¨è¿æ¥ ArXiv...")
    query = " OR ".join([f"cat:{cat}" for cat in TARGET_CATEGORIES])
    
    # æœç´¢æœ€è¿‘ 100 ç¯‡
    search = arxiv.Search(
        query=query,
        max_results=100, 
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )
    
    client_arxiv = arxiv.Client(page_size=100, delay_seconds=3, num_retries=3)
    new_papers = []
    
    # è®¾ç½®æ—¶é—´çª—å£ï¼šè¿‡å» 25 å°æ—¶ (GitHub Action æ¯å¤©è¿è¡Œä¸€æ¬¡ï¼Œç¨å¾®å¤šæŠ“ä¸€ç‚¹é˜²æ­¢æ¼æ‰)
    utc_now = datetime.datetime.now(datetime.timezone.utc)
    time_threshold = utc_now - datetime.timedelta(hours=25)

    for r in client_arxiv.results(search):
        paper_id = r.entry_id.split('/')[-1]
        
        # 1. å¦‚æœè®ºæ–‡å¤ªæ—§ï¼Œåœæ­¢å¤„ç† (å› ä¸ºæ˜¯æŒ‰æ—¶é—´å€’åºçš„)
        if r.updated < time_threshold:
            break
            
        # 2. å¦‚æœ ID å·²å­˜åœ¨ï¼Œè·³è¿‡
        if paper_id in existing_ids:
            continue
            
        # 3. å†æ¬¡ç¡®è®¤åˆ†ç±» (ArXiv æœç´¢æœ‰æ—¶ä¸ç²¾å‡†)
        cats = [c for c in r.categories if c in TARGET_CATEGORIES]
        if not cats:
            continue

        new_papers.append(r)

    print(f"ğŸ” å‘ç° {len(new_papers)} ç¯‡æ–°è®ºæ–‡éœ€è¦å¤„ç†ã€‚")
    return new_papers

def ai_process(title, abstract):
    """è°ƒç”¨ DeepSeek è¿›è¡Œæ€»ç»“å’Œç¿»è¯‘"""
    prompt = f"""
    è¯·å¤„ç†ä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼š
    Title: {title}
    Abstract: {abstract}

    ä»»åŠ¡ï¼š
    1. Innovation: ç”¨ä¸­æ–‡ä¸€å¥è¯æ€»ç»“æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆ<50å­—ï¼‰ã€‚
    2. Abstract_zh: å°†æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ã€‚

    è¯·ä»…è¿”å›åˆæ³•çš„ JSON æ ¼å¼ï¼š
    {{ "innovation": "...", "abstract_zh": "..." }}
    """
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "You are a JSON generator."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}, # å¼ºåˆ¶ JSON æ¨¡å¼
            temperature=0.1
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"âš ï¸ AI å¤„ç†å‡ºé”™: {e}")
        return {"innovation": "AI æ€»ç»“æš‚ä¸å¯ç”¨", "abstract_zh": "ç¿»è¯‘æš‚ä¸å¯ç”¨"}

# å°† daily_fetch.py ä¸­çš„ main å‡½æ•°æ›¿æ¢ä¸ºè¿™ä¸ªï¼š

def main():
    existing_data = get_existing_papers()
    existing_ids = {p['id'] for p in existing_data}
    
    # å°è¯•è·å–æ–°æ•°æ®
    try:
        raw_papers = fetch_arxiv_updates(existing_ids)
    except Exception as e:
        print(f"âŒ è·å– ArXiv æ•°æ®å‡ºé”™: {e}")
        raw_papers = []

    processed_list = []
    
    # å¦‚æœæœ‰æ–°è®ºæ–‡ï¼Œè¿›è¡Œå¤„ç†
    if raw_papers:
        for i, r in enumerate(raw_papers):
            print(f"[{i+1}/{len(raw_papers)}] å¤„ç†ä¸­: {r.title[:30]}...")
            
            ai_res = ai_process(r.title, r.summary)
            
            paper_obj = {
                "id": r.entry_id.split('/')[-1],
                "orig_title": r.title.replace('\n', ' '),
                "tags": [t.split('.')[-1] for t in r.categories if t in TARGET_CATEGORIES],
                "userTags": [],
                "coverGradient": random.choice(GRADIENTS),
                "summary": { "innovation": ai_res.get("innovation", "æ— æ€»ç»“") },
                "abstract_zh": ai_res.get("abstract_zh", "æ— ç¿»è¯‘"),
                "abstract_en": r.summary.replace('\n', ' '),
                "authors": [a.name for a in r.authors[:5]],
                "affiliation": r.categories[0], 
                "date": r.updated.strftime("%Y-%m-%d"),
                "pdf_url": r.pdf_url,
                "likes": 0, "isLiked": False, 
                "collections": 0, "isCollected": False,
                "comments": [], "qa": [],
                "expanded": False, "isTranslated": False
            }
            processed_list.append(paper_obj)
            time.sleep(1) # é¿å…è§¦å‘ API é€Ÿç‡é™åˆ¶
    else:
        print("âš ï¸ æœ¬æ¬¡æ²¡æœ‰å‘ç°æ–°è®ºæ–‡ã€‚")

    # --- å…³é”®ä¿®æ”¹ï¼šæ— è®ºæœ‰æ²¡æœ‰æ–°è®ºæ–‡ï¼Œéƒ½æ‰§è¡Œåˆå¹¶ä¸ä¿å­˜ ---
    
    # åˆå¹¶ï¼šæ–°è®ºæ–‡æ”¾æœ€å‰é¢
    final_data = processed_list + existing_data
    
    # å¼ºåˆ¶ä¿å­˜ï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿™é‡Œä¼šåˆ›å»ºå®ƒï¼›å¦‚æœå­˜åœ¨ï¼Œä¼šæ›´æ–°å®ƒï¼‰
    with open(JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ æ•°æ®å¤„ç†å®Œæˆã€‚æ–‡ä»¶å·²ä¿å­˜è‡³ {JSON_FILE} (å½“å‰æ€»æ•°: {len(final_data)})")
    
    print(f"ğŸ’¾ æ›´æ–°å®Œæˆï¼Œæ–°å¢ {len(processed_list)} ç¯‡ã€‚")

if __name__ == "__main__":
    main()
